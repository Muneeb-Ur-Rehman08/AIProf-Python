<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Streaming Voice Assistant Demo (WAV via ScriptProcessor)</title>
    <style>
      #messages {
        border: 1px solid #ccc;
        padding: 1em;
        width: 400px;
        height: 300px;
        overflow-y: auto;
      }
      .assistant-stream {
        color: gray;
        font-style: italic;
      }
      .typing-indicator {
        display: inline-block;
        margin-left: 5px;
      }
      .typing-indicator span {
        display: inline-block;
        width: 8px;
        height: 8px;
        background-color: #90949c;
        border-radius: 50%;
        margin-right: 5px;
        animation: typing 1s infinite ease-in-out;
      }
      .typing-indicator span:nth-child(2) { animation-delay: 0.2s; }
      .typing-indicator span:nth-child(3) { animation-delay: 0.4s; }
      @keyframes typing {
        0%, 100% { transform: translateY(0); }
        50%      { transform: translateY(-10px); }
      }
    </style>
  </head>
  <body>
    <h1>Voice Assistant (Streaming, WAV)</h1>
    <button id="startBtn">Start Conversation</button>
    <button id="stopBtn" disabled>Stop Listening</button>
    <div id="messages"></div>

  
    <script>
      // =======================
      // Global Variables
      // =======================
      let socket;
      
      const testsocket = new WebSocket("wss://aiprof-preprod.up.railway.app/ws/assistant/");
      testsocket.onopen = () => console.log("Connected");
      testsocket.onerror = (error) => console.log("WebSocket Error:", error);



      // Recording-related variables
      let audioContext;       // For recording
      let analyser;
      let scriptProcessorNode;
      let sourceNode;
      let mediaStream;
      let leftChannelData = [];
      let recordingLength = 0;
      let sampleRate = 44100; // Updated from AudioContext

      let isRecording = false;
      let isSpeaking = false;
      let silenceTimer;

      // Playback-related variables
      let audioQueue = [];
      let isPlaying = false;
      let playbackAudioContext;  // Reused for all audio playback

      // Streaming text message element
      let currentStreamingDiv = null;

      // Configuration constants
      const SILENCE_DURATION = 3000;   // 3 seconds
      const AMPLITUDE_THRESHOLD = 10;    // Adjust based on your noise floor

      // Modified recording logic
      let chunkInterval;
      const CHUNK_DURATION = 5000; // 5 seconds

      // =======================
      // WebSocket Functions
      // =======================
      function initializeWebSocket() {
  // Close an existing socket if necessary
  if (socket) {
    socket.close();
  }

  // Get the current hostname
  const hostname = window.location.hostname;

  // Determine if we're on Railway or localhost
  const isRailway = hostname.includes('railway.app');

  // Set up the WebSocket URL
  let wsUrl;
  if (isRailway) {
    // For Railway: Use wss:// and the full hostname
    wsUrl = `wss://${hostname}/ws/assistant/`;
  } else {
    // For localhost: Use the current protocol (ws/wss) and port
    const protocol = window.location.protocol === "https:" ? "wss" : "ws";
    const port = window.location.port ? `:${window.location.port}` : '';
    wsUrl = `${protocol}://${hostname}${port}/ws/assistant/`;
  }

  console.log("Connecting to WebSocket:", wsUrl);
  socket = new WebSocket(wsUrl);

  socket.onopen = () => {
    console.log("WebSocket connected");
    socket.send(JSON.stringify({ control: "start_conversation" }));
    startRecording();
  };

  socket.onmessage = async (event) => {
    const data = JSON.parse(event.data);
    console.log("Received message:", data);

    if (data.type === "assistant_response") {
      if (data.message) {
        addMessage(data.message);
      }
      if (!isPlaying && data.audio_data) {
        playAudio(data.audio_data);
      }
    }
  };

  socket.onclose = () => {
    console.log("WebSocket disconnected");
    document.getElementById("startBtn").disabled = false;
    document.getElementById("stopBtn").disabled = true;
    stopRecording();
  };

  socket.onerror = (error) => {
    console.error("WebSocket error:", error);
    alert("Error connecting to server. Please try again.");
  };
}

      
      // =======================
      // Recording Functions
      // =======================
      async function startRecording() {
        // Only start recording if the socket is open
        if (!socket || socket.readyState !== WebSocket.OPEN) {
          console.log("Socket is not open. Cannot start recording.");
          return;
        }
        // Guard against multiple recordings
        if (isRecording) return;
        
        // Reset buffers
        leftChannelData = [];
        recordingLength = 0;
        
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        } catch (err) {
          console.error("Error accessing microphone:", err);
          addMessage("System", "Error accessing microphone: " + err.message);
          return;
        }
        
        // Create and configure the audio context for recording
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        sampleRate = audioContext.sampleRate;
        
        // Create audio nodes
        sourceNode = audioContext.createMediaStreamSource(mediaStream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 512;
        analyser.smoothingTimeConstant = 0.3;
        
        const bufferSize = 4096;
        scriptProcessorNode = audioContext.createScriptProcessor(bufferSize, 1, 1);
        scriptProcessorNode.onaudioprocess = (audioEvent) => {
          const inputData = audioEvent.inputBuffer.getChannelData(0);
          // Copy the input data
          leftChannelData.push(new Float32Array(inputData));
          recordingLength += inputData.length;
        };
        
        // Connect nodes
        sourceNode.connect(analyser);
        sourceNode.connect(scriptProcessorNode);
        scriptProcessorNode.connect(audioContext.destination); // Needed to keep the processor alive
        
        isRecording = true;
        isSpeaking = false;
        monitorAudio();
        
        document.getElementById("startBtn").disabled = true;
        document.getElementById("stopBtn").disabled = false;
        addMessage("System", "Recording started...");

        // Send audio chunks every 5 seconds
        chunkInterval = setInterval(() => {
          if (leftChannelData.length > 0) {
            const merged = mergeBuffers(leftChannelData, recordingLength);
            const wavBlob = encodeWAV(merged, sampleRate);
            sendAudioToServer(wavBlob);
            // Reset buffers
            leftChannelData = [];
            recordingLength = 0;
          }
        }, CHUNK_DURATION);
      }

      function monitorAudio() {
        if (!isRecording || !analyser) return;
        
        const dataArray = new Uint8Array(analyser.fftSize);
        analyser.getByteTimeDomainData(dataArray);

        let sumOfSquares = 0;
        for (let i = 0; i < dataArray.length; i++) {
          const sample = dataArray[i] - 128;
          sumOfSquares += sample * sample;
        }
        const rms = Math.sqrt(sumOfSquares / dataArray.length);
        
        if (rms > AMPLITUDE_THRESHOLD) {
          // User is speaking
          if (!isSpeaking) {
            isSpeaking = true;
            if (silenceTimer) {
              clearTimeout(silenceTimer);
              silenceTimer = null;
            }
          }
        } else {
          // Silence detected; if speech had been detected, stop recording after a delay
          if (isSpeaking) {
            silenceTimer = setTimeout(() => {
              isSpeaking = false;
              stopRecording();
            }, SILENCE_DURATION);
          }
        }

        // Check if requestAnimationFrame is supported and working
        if (window.requestAnimationFrame) {
          requestAnimationFrame(monitorAudio);
        } else {
          // Fallback to setTimeout as a backup
          setTimeout(monitorAudio, 16); // ~60fps
        }
      }
      
      function stopRecording() {
        if (!isRecording) return;
        
        console.log("Stopping recording...");

        // Disconnect audio nodes and stop tracks
        if (scriptProcessorNode) scriptProcessorNode.disconnect();
        if (sourceNode) sourceNode.disconnect();
        if (analyser) analyser.disconnect();
        if (mediaStream) {
          mediaStream.getTracks().forEach(track => track.stop());
        }
        if (audioContext) {
          audioContext.close();
        }
        
        isRecording = false;
        isSpeaking = false;
        if (silenceTimer) {
          clearTimeout(silenceTimer);
          silenceTimer = null;
        }
        
        addMessage("System", "Recording stopped");
        
        // If any audio was recorded, merge and send it to the server
        if (recordingLength > 0) {
          const mergedBuffer = mergeBuffers(leftChannelData, recordingLength);
          const wavBlob = encodeWAV(mergedBuffer, sampleRate);
          // sendAudioToServer(wavBlob);
        }
        
        // Reset recording buffers
        leftChannelData = [];
        recordingLength = 0;
        clearInterval(chunkInterval);
      }
      
      // Merge an array of Float32Arrays into one
      function mergeBuffers(bufferList, totalLength) {
        const result = new Float32Array(totalLength);
        let offset = 0;
        for (const buffer of bufferList) {
          result.set(buffer, offset);
          offset += buffer.length;
        }
        return result;
      }
      
      // Encode PCM samples into a 16-bit WAV Blob
      function encodeWAV(samples, sampleRate) {
        const bytesPerSample = 2;
        const numChannels = 1;
        const blockAlign = numChannels * bytesPerSample;
        const buffer = new ArrayBuffer(44 + samples.length * bytesPerSample);
        const view = new DataView(buffer);
        
        writeString(view, 0, "RIFF");
        view.setUint32(4, 36 + samples.length * bytesPerSample, true);
        writeString(view, 8, "WAVE");
        writeString(view, 12, "fmt ");
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * blockAlign, true);
        view.setUint16(32, blockAlign, true);
        view.setUint16(34, 16, true);
        writeString(view, 36, "data");
        view.setUint32(40, samples.length * bytesPerSample, true);
        
        let index = 44;
        for (let i = 0; i < samples.length; i++) {
          let sample = samples[i];
          // Clamp the sample
          sample = Math.max(-1, Math.min(1, sample));
          // Scale to 16-bit signed integer
          sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
          view.setInt16(index, sample, true);
          index += 2;
        }
        
        return new Blob([view], { type: "audio/wav" });
      }
      
      // Write a string into the DataView at the given offset
      function writeString(view, offset, str) {
        for (let i = 0; i < str.length; i++) {
          view.setUint8(offset + i, str.charCodeAt(i));
        }
      }

      let playAudioContext;
      let playSource;

      async function playAudio(audioData) {
        playAudioContext = new (window.AudioContext || window.webkitAudioContext)();

        // Only try to stop if playSource exists and has been started
        if (playSource && isPlaying) {
          playSource.stop();
        }

        playSource = playAudioContext.createBufferSource();

        // Convert audioData to ArrayBuffer if it's not already
        const arrayBuffer = audioData instanceof ArrayBuffer ? audioData : base64ToArrayBuffer(audioData);

        const decodedBuffer = await playAudioContext.decodeAudioData(arrayBuffer);
        playSource.buffer = decodedBuffer;
        playSource.connect(playAudioContext.destination);
        isPlaying = true;
        playSource.start(0);

        startRecording()

        playSource.onended = () => {
          isPlaying = false;
          playSource = null;
          startRecording();
        };
      }

      function stopAudio() {
          if (playSource) {
              playSource.stop();
              playSource = null;
              isPlaying = false;

          }
          if (playAudioContext) {
              playAudioContext.close();
              playAudioContext = null;
          }

      }

      
      // Convert the WAV Blob to a base64 string and send it to the server

      function sendAudioToServer(wavBlob) {
        if (isPlaying) {
          // stop the audio to be played
          stopAudio();
        }
        const reader = new FileReader();
        reader.onloadend = () => {
          const base64Audio = reader.result.split(",")[1];
          if (socket && socket.readyState === WebSocket.OPEN) {
            socket.send(JSON.stringify({
              audio_data: base64Audio,
              format: "wav",
              interrupt: true
            }));
            // Start recording again after sending audio
            startRecording();
          }
        };
        reader.readAsDataURL(wavBlob);
      }
      
      // =======================
      // Playback Functions
      // =======================
      async function playNextAudioChunk() {
        // When there are no more chunks in the queue,
        // mark playback as finished and restart recording if the socket is still open.
        if (audioQueue.length === 0) {
          isPlaying = false;
          if (socket && socket.readyState === WebSocket.OPEN && !isRecording) {
            addMessage("System", "Playback complete. Resuming recording...");
            startRecording();
          }
          return;
        }
        
        isPlaying = true;
        // Remove the first chunk from the queue
        const audioBase64 = audioQueue.shift();
        
        try {
          // Create or reuse the playback AudioContext
          if (!playbackAudioContext || playbackAudioContext.state === "closed") {
            playbackAudioContext = new (window.AudioContext || window.webkitAudioContext)();
          }
          
          const rawData = base64ToArrayBuffer(audioBase64);
          // Wrap raw PCM data with a WAV header.
          const wavBuffer = encodePCMToWAV(rawData, {
            sampleRate: 16000, // Ensure this matches your backend sample rate
            numChannels: 1,
            bitDepth: 16
          });
          
          const decodedBuffer = await playbackAudioContext.decodeAudioData(wavBuffer);
          const source = playbackAudioContext.createBufferSource();
          source.buffer = decodedBuffer;
          source.connect(playbackAudioContext.destination);
          
          source.start(0);
          source.onended = () => {
            playNextAudioChunk();
          };
        } catch (error) {
          console.error("Audio playback error:", error);
          playNextAudioChunk();
        }
      }
      
      // Convert a base64 string to an ArrayBuffer
      function base64ToArrayBuffer(base64) {
        const binaryString = atob(base64);
        const len = binaryString.length;
        const bytes = new Uint8Array(len);
        for (let i = 0; i < len; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        return bytes.buffer;
      }
      
      // Wrap raw PCM data with a WAV header.
      // (This is used for playback since the server sends raw PCM as base64)
      function encodePCMToWAV(pcmData, options) {
        const header = new ArrayBuffer(44);
        const view = new DataView(header);
        
        writeString(view, 0, "RIFF");
        view.setUint32(4, 36 + pcmData.byteLength, true);
        writeString(view, 8, "WAVE");
        writeString(view, 12, "fmt ");
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true); // PCM format
        view.setUint16(22, options.numChannels, true);
        view.setUint32(24, options.sampleRate, true);
        view.setUint32(28, options.sampleRate * options.numChannels * (options.bitDepth / 8), true);
        view.setUint16(32, options.numChannels * (options.bitDepth / 8), true);
        view.setUint16(34, options.bitDepth, true);
        writeString(view, 36, "data");
        view.setUint32(40, pcmData.byteLength, true);
        
        // Combine header and PCM data
        const wavBuffer = new ArrayBuffer(44 + pcmData.byteLength);
        const wavView = new Uint8Array(wavBuffer);
        wavView.set(new Uint8Array(header), 0);
        wavView.set(new Uint8Array(pcmData), 44);
        
        return wavBuffer;
      }
      
      // =======================
      // UI Update Functions
      // =======================
      function addMessage(sender, text) {
        const messagesDiv = document.getElementById("messages");
        const msg = document.createElement("div");
        msg.className = sender.toLowerCase();
        msg.innerHTML = `<strong>${sender}:</strong> ${text}`;
        messagesDiv.appendChild(msg);
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
      }
      
      // Append partial text from the assistant along with a typing indicator
      function appendStreamingMessage(chunk) {
        if (!chunk) return;
        const messagesDiv = document.getElementById("messages");
        
        if (!currentStreamingDiv) {
          currentStreamingDiv = document.createElement("div");
          currentStreamingDiv.className = "assistant-stream";
          currentStreamingDiv.innerHTML = `<strong>Assistant:</strong> `;
          
          const typingIndicator = document.createElement("div");
          typingIndicator.className = "typing-indicator";
          typingIndicator.innerHTML = `<span></span><span></span><span></span>`;
          currentStreamingDiv.appendChild(typingIndicator);
          
          messagesDiv.appendChild(currentStreamingDiv);
        }
        
        // Remove the old typing indicator before appending new text
        const oldIndicator = currentStreamingDiv.querySelector(".typing-indicator");
        if (oldIndicator) {
          oldIndicator.remove();
        }
        
        currentStreamingDiv.appendChild(document.createTextNode(chunk));
        
        // Re-add a typing indicator at the end
        const newTypingIndicator = document.createElement("div");
        newTypingIndicator.className = "typing-indicator";
        newTypingIndicator.innerHTML = `<span></span><span></span><span></span>`;
        currentStreamingDiv.appendChild(newTypingIndicator);
        
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
      }
      
      // Finalize the streaming message by removing the typing indicator.
      function finalizeStreamingMessage(finalText) {
        if (currentStreamingDiv) {
          const indicator = currentStreamingDiv.querySelector(".typing-indicator");
          if (indicator) indicator.remove();
          currentStreamingDiv = null;
        }
        if (finalText) {
          addMessage("Assistant", finalText);
        }
      }
      
      function updateVadStatus(data) {
        const statusElement = document.createElement('div');
        statusElement.className = `vad-status ${data.status}`;
        statusElement.textContent = data.message;
        document.getElementById('messages').appendChild(statusElement);
      }
      
      // =======================
      // Event Listeners
      // =======================
      document.getElementById("startBtn").addEventListener("click", () => {
        initializeWebSocket();
      });
      
      document.getElementById("stopBtn").addEventListener("click", () => {
        // Closing the socket will trigger the onclose handler and stop recording.
        if (socket) {
          socket.close();
        }
      });
      
      window.addEventListener("beforeunload", () => {
        if (socket) socket.close();
      });

      function stopAudioPlayback() {
        const audioElement = document.getElementById('myAudio');
        audioElement.pause();
        audioElement.currentTime = 0;
      }
    </script>
  </body>
</html>