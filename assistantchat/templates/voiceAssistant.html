<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Streaming Voice Assistant Demo (WAV via ScriptProcessor)</title>
    <style>
      #messages {
        border: 1px solid #ccc;
        padding: 1em;
        width: 400px;
        height: 300px;
        overflow-y: auto;
      }
      .assistant-stream {
        color: gray;
        font-style: italic;
      }
      .typing-indicator {
        display: inline-block;
        margin-left: 5px;
      }
      .typing-indicator span {
        display: inline-block;
        width: 8px;
        height: 8px;
        background-color: #90949c;
        border-radius: 50%;
        margin-right: 5px;
        animation: typing 1s infinite ease-in-out;
      }
      .typing-indicator span:nth-child(2) {
        animation-delay: 0.2s;
      }
      .typing-indicator span:nth-child(3) {
        animation-delay: 0.4s;
      }
      @keyframes typing {
        0%,
        100% {
          transform: translateY(0);
        }
        50% {
          transform: translateY(-10px);
        }
      }
    </style>
  </head>
  <body>
    <h1>Voice Assistant (Streaming, WAV)</h1>
    <button id="startBtn">Start Conversation</button>
    <button id="stopBtn" disabled>Stop Listening</button>
    <div id="messages"></div>

    <script>
      let socket;

      // --- Audio / Recording State ---
      let audioContext;
      let analyser;
      let scriptProcessorNode;
      let sourceNode;
      let mediaStream;

      // Buffers for raw audio (mono)
      let leftChannelData = [];
      let recordingLength = 0;
      let sampleRate = 44100; // will get from AudioContext

      let isRecording = false;
      let isSpeaking = false;
      let silenceTimer;

      const SILENCE_DURATION = 3000; // 3 seconds
      const AMPLITUDE_THRESHOLD = 10; // tweak this for your noise floor

      function initializeWebSocket() {
        if (socket) {
          socket.close();
        }

        const protocol = window.location.protocol === "https:" ? "wss" : "ws";
        socket = new WebSocket(`${protocol}://${window.location.host}/ws/assistant/`);

        socket.onopen = () => {
          console.log("WebSocket connected");
          socket.send(JSON.stringify({ control: "start_conversation" }));
          startRecording();
        };

        socket.onclose = () => {
          console.log("WebSocket disconnected");
          document.getElementById("startBtn").disabled = false;
          document.getElementById("stopBtn").disabled = true;
          stopRecording();
        };

        socket.onerror = (error) => {
          console.error("WebSocket error:", error);
          alert("Error connecting to server. Please try again.");
        };

        socket.onmessage = async (event) => {
          const data = JSON.parse(event.data);
          console.log("Received message:", data);

          if (data.type === "assistant_stream") {
            // Handle streaming partial text
            appendStreamingMessage(data.chunk);
          } else if (data.type === "assistant_response") {
            // Final response with optional TTS audio
            finalizeStreamingMessage(data.message);
            if (data.audio_data) {
              await playAudioData(data.audio_data);
              // Optionally auto-restart recording here:
              startRecording();
            }
          }
        };
      }

      async function startRecording() {
        if (isRecording) return;

        // Reset data
        leftChannelData = [];
        recordingLength = 0;

        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        } catch (err) {
          console.error("Error accessing microphone:", err);
          addMessage("System", "Error accessing microphone: " + err.message);
          return;
        }

        // AudioContext
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        sampleRate = audioContext.sampleRate;

        // Create nodes
        sourceNode = audioContext.createMediaStreamSource(mediaStream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 512;
        analyser.smoothingTimeConstant = 0.3;

        const bufferSize = 4096; // power of 2 up to 16384
        scriptProcessorNode = audioContext.createScriptProcessor(bufferSize, 1, 1);
        scriptProcessorNode.onaudioprocess = (audioEvent) => {
          const inputBuffer = audioEvent.inputBuffer;
          const inputData = inputBuffer.getChannelData(0);
          // clone the data
          leftChannelData.push(new Float32Array(inputData));
          recordingLength += inputData.length;
        };

        // Connect for capturing + analyzing
        sourceNode.connect(analyser);
        sourceNode.connect(scriptProcessorNode);
        scriptProcessorNode.connect(audioContext.destination); // keep processor running

        isRecording = true;
        isSpeaking = false;

        // Start monitoring amplitude
        monitorAudio();

        document.getElementById("startBtn").disabled = true;
        document.getElementById("stopBtn").disabled = false;
        addMessage("System", "Recording started...");
      }

      function monitorAudio() {
        if (!isRecording || !analyser) return;

        const dataArray = new Uint8Array(analyser.fftSize);
        analyser.getByteTimeDomainData(dataArray);

        // Compute RMS
        let sumOfSquares = 0;
        for (let i = 0; i < dataArray.length; i++) {
          const centeredSample = dataArray[i] - 128;
          sumOfSquares += centeredSample * centeredSample;
        }
        const rms = Math.sqrt(sumOfSquares / dataArray.length);

        if (rms > AMPLITUDE_THRESHOLD) {
          // speaking
          if (!isSpeaking) {
            isSpeaking = true;
            if (silenceTimer) clearTimeout(silenceTimer);
          }
        } else {
          // silence
          if (isSpeaking) {
            silenceTimer = setTimeout(() => {
              isSpeaking = false;
              // finalize audio chunk on silence
              stopRecording(); // will handle encode + send
            }, SILENCE_DURATION);
          }
        }

        requestAnimationFrame(monitorAudio);
      }

      function stopRecording() {
        if (!isRecording) return;

        // Cleanup audio
        if (scriptProcessorNode) scriptProcessorNode.disconnect();
        if (sourceNode) sourceNode.disconnect();
        if (analyser) analyser.disconnect();

        if (mediaStream) {
          mediaStream.getTracks().forEach((track) => track.stop());
        }

        if (audioContext) {
          audioContext.close();
        }

        isRecording = false;
        isSpeaking = false;
        clearTimeout(silenceTimer);

        addMessage("System", "Recording stopped");

        // We now have all the raw PCM data in leftChannelData.
        if (recordingLength > 0) {
          const mergedBuffer = mergeBuffers(leftChannelData, recordingLength);
          const wavBlob = encodeWAV(mergedBuffer, sampleRate);

          // Convert to base64 and send
          sendAudioToServer(wavBlob);
        }

        // reset buffers
        leftChannelData = [];
        recordingLength = 0;
      }

      // Merge Float32 chunks
      function mergeBuffers(channelBufferList, totalLength) {
        const result = new Float32Array(totalLength);
        let offset = 0;
        for (let i = 0; i < channelBufferList.length; i++) {
          const buffer = channelBufferList[i];
          result.set(buffer, offset);
          offset += buffer.length;
        }
        return result;
      }

      // Encode raw float samples as 16-bit WAV
      function encodeWAV(samples, sampleRate) {
        const bytesPerSample = 2;
        const numChannels = 1;
        const blockAlign = numChannels * bytesPerSample;
        const buffer = new ArrayBuffer(44 + samples.length * bytesPerSample);
        const view = new DataView(buffer);

        /* RIFF identifier */
        writeString(view, 0, "RIFF");
        /* file length */
        view.setUint32(4, 36 + samples.length * bytesPerSample, true);
        /* RIFF type */
        writeString(view, 8, "WAVE");
        /* format chunk identifier */
        writeString(view, 12, "fmt ");
        /* format chunk length */
        view.setUint32(16, 16, true);
        /* sample format (raw) */
        view.setUint16(20, 1, true);
        /* channel count */
        view.setUint16(22, numChannels, true);
        /* sample rate */
        view.setUint32(24, sampleRate, true);
        /* byte rate (sample rate * block align) */
        view.setUint32(28, sampleRate * blockAlign, true);
        /* block align (channel count * bytes/sample) */
        view.setUint16(32, blockAlign, true);
        /* bits per sample */
        view.setUint16(34, 16, true);
        /* data chunk identifier */
        writeString(view, 36, "data");
        /* data chunk length */
        view.setUint32(40, samples.length * bytesPerSample, true);

        // PCM samples
        let index = 44;
        for (let i = 0; i < samples.length; i++) {
          let sample = samples[i];
          // clamp
          if (sample > 1) sample = 1;
          if (sample < -1) sample = -1;
          // scale to 16-bit
          sample = sample < 0 ? sample * 0x8000 : sample * 0x7fff;
          view.setInt16(index, sample, true);
          index += 2;
        }

        return new Blob([view], { type: "audio/wav" });
      }

      function writeString(view, offset, str) {
        for (let i = 0; i < str.length; i++) {
          view.setUint8(offset + i, str.charCodeAt(i));
        }
      }

      function sendAudioToServer(wavBlob) {
        // Convert Blob to base64
        const reader = new FileReader();
        reader.readAsDataURL(wavBlob);
        reader.onloadend = () => {
          const base64Audio = reader.result.split(",")[1];
          if (socket && socket.readyState === WebSocket.OPEN) {
            socket.send(
              JSON.stringify({
                audio_data: base64Audio,
                format: "wav",
              })
            );
          }
        };
      }

      // For playing back TTS from server (already base64-encoded WAV or other)
      async function playAudioData(audioBase64) {
        return new Promise((resolve) => {
          const ctx = new (window.AudioContext || window.webkitAudioContext)();
          const audioData = base64ToArrayBuffer(audioBase64);

          ctx
            .decodeAudioData(audioData, (buffer) => {
              const source = ctx.createBufferSource();
              source.buffer = buffer;
              source.connect(ctx.destination);
              source.onended = resolve;
              source.start(0);
            })
            .catch((error) => {
              console.error("Error decoding audio:", error);
              resolve();
            });
        });
      }

      function base64ToArrayBuffer(base64) {
        const binaryString = window.atob(base64);
        const length = binaryString.length;
        const bytes = new Uint8Array(length);
        for (let i = 0; i < length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        return bytes.buffer;
      }

      document.getElementById("startBtn").addEventListener("click", () => {
        initializeWebSocket();
      });

      document.getElementById("stopBtn").addEventListener("click", () => {
        stopRecording();
        if (socket) socket.close();
        addMessage("System", "Recording stopped");
        document.getElementById("startBtn").disabled = false;
        document.getElementById("stopBtn").disabled = true;
      });

      function addMessage(sender, text) {
        const messagesDiv = document.getElementById("messages");
        const msg = document.createElement("div");
        msg.className = sender.toLowerCase();
        msg.innerHTML = `<strong>${sender}:</strong> ${text}`;
        messagesDiv.appendChild(msg);
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
      }

      let currentStreamingDiv = null;
      function appendStreamingMessage(chunk) {
        const messagesDiv = document.getElementById("messages");

        // Create new container if none
        if (!currentStreamingDiv) {
          currentStreamingDiv = document.createElement("div");
          currentStreamingDiv.className = "assistant-stream";
          currentStreamingDiv.innerHTML = `<strong>Assistant:</strong> `;

          // add typing indicator
          const typingIndicator = document.createElement("div");
          typingIndicator.className = "typing-indicator";
          typingIndicator.innerHTML = `<span></span><span></span><span></span>`;
          currentStreamingDiv.appendChild(typingIndicator);

          messagesDiv.appendChild(currentStreamingDiv);
        }

        // remove old typing
        const oldIndicator = currentStreamingDiv.querySelector(".typing-indicator");
        if (oldIndicator) {
          oldIndicator.remove();
        }

        // append new chunk
        const textNode = document.createTextNode(chunk);
        currentStreamingDiv.appendChild(textNode);

        // add typing back
        const newTypingIndicator = document.createElement("div");
        newTypingIndicator.className = "typing-indicator";
        newTypingIndicator.innerHTML = `<span></span><span></span><span></span>`;
        currentStreamingDiv.appendChild(newTypingIndicator);

        messagesDiv.scrollTop = messagesDiv.scrollHeight;
      }

      function finalizeStreamingMessage(finalText) {
        if (currentStreamingDiv) {
          const typingIndicator = currentStreamingDiv.querySelector(".typing-indicator");
          if (typingIndicator) typingIndicator.remove();

          currentStreamingDiv = null;
        }
        // Optionally add final text as a normal “Assistant” message:
        if (finalText) {
          addMessage("Assistant", finalText);
        }
      }

      window.addEventListener("beforeunload", () => {
        if (socket) socket.close();
      });
    </script>
  </body>
</html>
